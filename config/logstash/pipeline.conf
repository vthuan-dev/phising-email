# config/logstash/pipeline.conf
input {
  beats {
    port => 5044
  }
}

filter {
  # Parse JSON from Filebeat
  if [logtype] == "email_events" {
    # Ensure proper field types
    mutate {
      convert => {
        "ml_score" => "float"
        "llm_confidence" => "float"
        "llm_latency_ms" => "integer"
        "suspicious_keywords" => "integer"
        "url_in_blacklist" => "boolean"
      }
    }

    # Parse timestamp
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }

    # Add sender domain extraction
    if [sender] {
      grok {
        match => { "sender" => "(?<sender_domain>[^@]+@(?<sender_domain_only>[^@]+))" }
        add_field => { "sender_domain" => "%{sender_domain_only}" }
        tag_on_failure => ["_sender_parse_failed"]
      }
    }

    # Create subject hash for deduplication
    if [subject] {
      fingerprint {
        source => "subject"
        target => "subject_hash"
        method => "SHA256"
      }
    }

    # GeoIP lookup if sender_ip exists
    if [sender_ip] and [sender_ip] != "" {
      geoip {
        source => "sender_ip"
        target => "geoip"
        add_tag => ["geoip"]
      }
    }

    # Add processing timestamp
    mutate {
      add_field => { "processed_at" => "%{@timestamp}" }
    }

    # Truncate long fields to prevent mapping explosion
    if [llm_explanation] {
      truncate {
        fields => ["llm_explanation"]
        length_bytes => 1000
      }
    }

    if [body_excerpt] {
      truncate {
        fields => ["body_excerpt"]
        length_bytes => 2000
      }
    }

    # Add risk level based on scores
    ruby {
      code => "
        ml_score = event.get('ml_score').to_f
        llm_confidence = event.get('llm_confidence').to_f
        
        if ml_score >= 0.85 or llm_confidence >= 0.85
          event.set('risk_level', 'high')
        elsif ml_score >= 0.6 or llm_confidence >= 0.6
          event.set('risk_level', 'medium')
        else
          event.set('risk_level', 'low')
        end
      "
    }

    # Remove temporary fields
    mutate {
      remove_field => ["host", "agent", "@version", "input", "ecs", "log"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "phish-mail-%{+YYYY.MM.dd}"
    template_name => "phish-mail"
    template_pattern => "phish-mail-*"
    template_overwrite => true
    template => "/usr/share/logstash/templates/phish-mail.json"
  }

  # Debug output (remove in production)
  stdout {
    codec => rubydebug
  }
}
